#' @title LINCS method for GESS
#' @description 
#' It uses query signature to search against the reference database in the 
#' \code{qSig} by LINCS method, which is an implementation of the method from 
#' Subramanian et al, 2017.
#' @details 
#' Subramanian et al., 2017 introduced a new GESS method (\code{LINCS}) that is 
#' similar with the CMAP method but weights genes in the query set by using 
#' expression profiles with scores instead of rank transformed as reference 
#' database. Since now, no implementations of this algorithm available, it is 
#' implemented here. 
#' 
#' The LINCS method takes about 1.7 min on a single CPU core for querying 
#' with a single signature against 10,000 signatures in the LINCS database.
#' 
#' Description of the score columns in the gess_lincs tibble result:
#' \itemize{
#'   \item WTCS: Weighted Connectivity Score, a bi-directional Enrichment 
#'   Score of up and down query set. If ES of up and down sets are of different
#'   signs, WTCS is (ESup-ESdown)/2, otherwise, it is 0. WTCS ranges between 
#'   −1 and 1. It will be positive for signatures that are positively related 
#'   and negative for those that are inversely related, and near zero for 
#'   signatures that are unrelated.
#'   \item WTCS_Pval: Nominal p-value of WTCS computed by comparing WTCS 
#'   against WTCS null distribution obtained from random queries
#'   \item WTCS_FDR: false discovery rate of WTCS P value
#'   \item NCS: To make connectivity scores comparable across cell types and 
#'   perturbation types, the scores are normalized. Given a vector of WTCS 
#'   values w resulting from a query, the values are normalized within each 
#'   cell line c and perturbagen type t to obtain Normalized Connectivity Scores
#'    (NCS) by dividing the WTCS score with signed mean of WTCS scores within 
#'   the subset of signatures in reference database corresponding to c and t.
#'   \item Tau: Tau score compares observed NCS to all others in Qref (All 
#'   query signatures in reference database). It represents the percentage of 
#'   reference queries with a lower |NCS| than |NCSq,r|, adjusted to retain 
#'   the sign of NCSq,r. NCSq,r is the normalized connectivity score for 
#'   signature r relative to query q. A tau of 90 indicates that only 10 
#'   percent of reference perturbations showed stronger connectivity to the 
#'   query. 
#'   \item TauRefSize: Size of reference perturbations for computing Tau.
#'   \item NCSct: Summarized NCS across cell types. Given a vector of NCS
#'   for perturbagen p, relative to query q, across all cell lines c in which p 
#'   was profiled, a cell-summarized connectivity score is obtained using a 
#'   maximum quantile statistic. It compares the 67 and 33 quantiles of 
#'   NCSp,c and retains whichever is of higher absolute magnitude.
#' }
#' 
#' @param qSig `qSig` object, The 'gess_method' slot should be 'LINCS'
#' @param ES_NULL can be set as `Default`, representing the null distribution 
#' of WTCS. The null distribution is generated with random queries for 
#' computing nominal P-values for WTCS. If set as `Default`, it uses the 
#' null distribution that we generated by running 1000 random queries 
#' against LINCS database.
#' 
#' It can also be a character representing path to the ES_NULL file generated
#' from `randQueryES` function. 
#' But it might take 3 days running 1000 random queries against LINCS
#' database locally without parallelizing jobs. 
#' To save time, it is recommended to parallelize with BiocParallel 
#' to run on CPU cores of a computer cluster with a scheduler (e.g. Slurm). For
#' vignette of generating the ES null distribution with Slurm scheduler on a 
#' computer cluster, please refer to the vignette in `signatureSearch_data` 
#' package by running `browseVignettes("signatureSearch_data")` in R.
#' 
#' @param tau TRUE or FALSE, whether to compute the tau score. It is only 
#' meaningful when the "refdb" in "qSig" is LINCS since the reference
#' queries are from LINCS database.
#' @param sortby rank the GESS result by one of the following scores: 
#' `WTCS`, `NCS`, `Tau`, `NCSct` or `NA` 
#' @param chunk_size size of chunk per processing
#' @return \code{\link{gessResult}} object, containing drugs in the reference 
#' database ranked by their similarity to the query signature
#' @importFrom R.utils gunzip
#' @import HDF5Array
#' @import SummarizedExperiment
#' @seealso \code{\link{qSig}}, \code{\link{gessResult}}, \code{\link{gess}}
#' @references For detailed description of the LINCS method and scores, 
#' please refer to: Subramanian, A., Narayan, R., Corsello, S. M., Peck, D. D., 
#' Natoli, T. E., Lu, X., … Golub, T. R. (2017). A Next Generation 
#' Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. Cell, 
#' 171(6), 1437–1452.e17. \url{https://doi.org/10.1016/j.cell.2017.10.049}
#' @examples 
#' db_dir <- system.file("extdata", "sample_db", package = "signatureSearch")
#' sample_db <- loadHDF5SummarizedExperiment(db_dir)
#' ## get "vorinostat__SKB__trt_cp" signature drawn from sample databass
#' query_mat <- as.matrix(assay(sample_db[,"vorinostat__SKB__trt_cp"]))
#' query = as.numeric(query_mat); names(query) = rownames(query_mat)
#' upset <- head(names(query[order(-query)]), 150)
#' downset <- tail(names(query[order(-query)]), 150)
#' qsig_lincs <- qSig(qsig = list(upset=upset, downset=downset), 
#'                    gess_method = "LINCS", refdb = sample_db,
#'                    refdb_name="sample")
#' lincs <- gess_lincs(qsig_lincs, sortby="NCS")
#' result(lincs)
#' @export
gess_lincs <- function(qSig, ES_NULL="Default", tau=FALSE,
                       sortby="NCS", chunk_size=5000){
  if(!is(qSig, "qSig")) stop("The 'qSig' should be an object of 'qSig' class")
  #stopifnot(validObject(qSig))
  if(qSig@gess_method != "LINCS"){
    stop(paste("The 'gess_method' slot of 'qSig' should be 'LINCS'",
               "if using 'gess_lincs' function"))
  }
  upset <- qSig@qsig[[1]]
  downset <- qSig@qsig[[2]]
  se <- qSig@refdb 
  res <- lincsEnrich(se=se, upset=upset, downset=downset, ES_NULL=ES_NULL, 
                     tau=tau, sortby=sortby, chunk_size=chunk_size)
  # add target column
  target <- suppressMessages(get_targets(res$pert))
  res <- left_join(res, target, by=c("pert"="drug_name"))
  
  x <- gessResult(result = as_tibble(res),
                  qsig = qSig@qsig,
                  gess_method = qSig@gess_method,
                  refdb_name = qSig@refdb_name)
  return(x)
}

randQuerySets <- function(id_names, N_queries, set_length=150) {
  randset_names <- paste0("randset_", sprintf("%09d", seq_len(N_queries)))
  rand_query_list <- lapply(randset_names, function(x) {
    id_list <- sample(id_names, 2 * set_length)
    split(id_list, rep(c("up", "down"), each=set_length))
  })
  names(rand_query_list) <- randset_names
  return(rand_query_list)
}

#' @importFrom utils read.delim
#' @importFrom stats quantile
.lincsScores <- function(esout, upset, downset, minTauRefSize, 
                         ES_NULL="Default", tau=FALSE) {
  ## P-value and FDR for WTCS based on ESnull from random queries where 
  ## p-value = sum(ESrand > ES_obs)/Nrand
  if(ES_NULL != "Default") {
    WTCSnull <- read.delim(ES_NULL)
    } else {
    # download ES_NULL.txt and save it to cache
      fl <- download_data_file(url=
        "http://biocluster.ucr.edu/~yduan004/signatureSearch_data/ES_NULL.txt",
                               rname="ES_NULL")
      WTCSnull <- read.delim(fl) 
  }
  WTCSnull[WTCSnull[, "Freq"]==0, "Freq"] <- 1 
  # Add pseudo count of 1 where Freq is zero 
  myrounding <- max(nchar(as.character(WTCSnull[,"WTCS"]))) - 3 
  # Three because of dot and minus sign
  es_round <- round(as.numeric(esout), myrounding) 
  # Assures same rounding used for WTCSnull computation
  WTCS_pval <- vapply(es_round, function(x) {
    sum(WTCSnull[abs(WTCSnull[,"WTCS"]) > abs(x),"Freq"])/sum(WTCSnull[,"Freq"])
    }, FUN.VALUE = numeric(1))
  WTCS_fdr <- p.adjust(WTCS_pval, "fdr")
  ## Normalized connectivity score (NCS)
  grouping <- paste(gsub("^.*?__", "", names(esout)), 
                    as.character(ifelse(esout > 0, "up", "down")), sep="__")
  es_na <- as.numeric(esout)
  es_na[es_na == 0] <- NA 
  # eliminates zeros from mean calculation; zeros have high impact on NCS 
  # values due to their high frequency
  groupmean <- tapply(es_na, grouping, mean, na.rm=TRUE)
  groupmean[is.na(groupmean)] <- 0 
  # In case groups contain only zeros, NA/NaN values are introduced in mean 
  # calculation which are reset to zeros here
  groupmean[groupmean==0] <- 10^-12 
  # Set zeros (can be from non NAs) to small value to avoid devision by zero 
  ncs <- as.numeric(esout) / abs(groupmean[grouping]) 
  # without abs() sign of neg values would switch to pos
  ## Tau calculation requires reference NCS lookup DB
  ## performs: sign(ncs_query) * 100/N sum(abs(ncs_ref) < abs(ncs_query))
  if(isTRUE(tau)){
    # download taurefList.rds and save it to cache
    fl <- download_data_file(url=
      "http://biocluster.ucr.edu/~yduan004/signatureSearch_data/taurefList.rds",
                             rname="taurefList")
    taurefList9264 <- readRDS(fl)
    
    ncs_query <- ncs; names(ncs_query) <- names(esout)
    queryDB_refDB_match <- 
      unique(unlist(lapply(taurefList9264, rownames))) %in% names(ncs_query)
    if(!all(queryDB_refDB_match)) warning(
      paste0("QueryDB and tauRefDB differ by ", 
      round(100 * sum(!queryDB_refDB_match)/length(queryDB_refDB_match),1), 
      "% of their entries.",
      " Accurate tau computation requires close to 0% divergence."))
    ncs_query_list <- split(ncs_query, 
                            factor(gsub("^.*?__", "", names(ncs_query))))
    tau <- lapply(names(ncs_query_list), function(x) {
      tmpDF <- taurefList9264[[x]]
      ncs_query_match <- names(ncs_query_list[[x]])[names(ncs_query_list[[x]]) 
                                                    %in% rownames(tmpDF)]
      if(length(ncs_query_match)>0) {
        tmpDF <- tmpDF[ncs_query_match, , drop=FALSE]
        # sign(ncs_query_list[[x]]) * 100/ncol(tmpDF) * 
        # rowSums(abs(tmpDF)  < abs(ncs_query_list[[x]]))
        sign(ncs_query_list[[x]]) * 100/ncol(tmpDF) * 
          rowSums(abs(tmpDF) < abs(round(ncs_query_list[[x]], 2))) 
        # rounded as in ref db
      } else {
        NULL
      }
    })
    tau <- unlist(tau)
    tau <- tau[names(ncs_query)]
    tauRefSize <- vapply(taurefList9264, ncol, 
                         FUN.VALUE = integer(1))[gsub("^.*?__", "", names(tau))]
    tau[tauRefSize < minTauRefSize] <- NA
    ## Add by YD 
    rm(taurefList9264); gc()
  }
  ## Summary across cell lines (NCSct)
  ctgrouping <- gsub("__.*__", "__", names(esout))
  qmax <- tapply(ncs, ctgrouping, function(x) { 
    q <- quantile(x, probs=c(0.33, 0.67))
    ifelse(abs(q[2]) >= abs(q[1]), q[2], q[1])
  })
  qmax <- qmax[ctgrouping]
  ## Organize result in data.frame
  new <- as.data.frame(t(vapply(seq_along(esout), function(i)
    unlist(strsplit(as.character(names(esout)[i]), "__")),
    FUN.VALUE = character(3))), stringsAsFactors=FALSE)
  colnames(new) = c("pert", "cell", "type")
  if(isTRUE(tau)){
    resultDF <- data.frame(
      new, 
      trend = as.character(ifelse(esout > 0, "up", "down")),
      WTCS = as.numeric(esout), 
      WTCS_Pval = WTCS_pval,
      WTCS_FDR = WTCS_fdr, 
      NCS = ncs, 
      Tau = tau,
      TauRefSize=tauRefSize, 
      NCSct = qmax, 
      N_upset = length(upset),
      N_downset = length(downset), stringsAsFactors = FALSE)
  } else {
    resultDF <- data.frame(
      new, 
      trend = as.character(ifelse(esout > 0, "up", "down")),
      WTCS = as.numeric(esout), 
      WTCS_Pval = WTCS_pval,
      WTCS_FDR = WTCS_fdr, 
      NCS = ncs, 
      NCSct = qmax, 
      N_upset = length(upset),
      N_downset = length(downset), stringsAsFactors = FALSE)
  }
  row.names(resultDF) <- NULL
  return(resultDF)
}


## Define enrichment function according to Subramanian et al, 2005
## Note: query corresponds to gene set, here Q.
.enrichScore <- function(sigvec, Q, type) {
  ## Preprocess arguments
  L <- names(sigvec)
  R <- as.numeric(sigvec)
  N <- length(L)
  NH <- length(Q)
  Ns <- N - NH
  hit_index <- as.numeric(L %in% Q)
  miss_index <- 1 - hit_index
  R <- abs(R^type)
  ## Compute ES
  NR <- sum(R[hit_index == 1])
  ESvec <- cumsum((hit_index * R * 1/NR) - (miss_index * 1/Ns)) 
  ES <- ESvec[which.max(abs(ESvec))]
  return(ES)
}

#' @importFrom DelayedArray apply
lincsEnrich <- function(se, upset, downset, sortby="NCS", type=1, 
                        output="all", ES_NULL="Default", tau=FALSE,
                        minTauRefSize=500, chunk_size=5000) {
  mycolnames <- c("WTCS", "NCS", "Tau", "NCSct", "N_upset", "N_downset", NA)
  if(!any(mycolnames %in% sortby)) stop("Unsupported value assinged to sortby.")
  
  ## Run .enrichScore on upset and downset
  ## When both upset and downset are provided 
  dmat <- assay(se)
  ceil <- ceiling(ncol(dmat)/chunk_size)
  ESout=NULL
  for(i in seq_len(ceil)){
    dmat_sub <- dmat[,(chunk_size*(i-1)+1):min(chunk_size*i, ncol(dmat))]
    mat <- as(dmat_sub, "matrix")
    if(length(upset)>0 & length(downset)>0) {
      ESup <- apply(mat, 2, function(x) 
        .enrichScore(sigvec=sort(x, decreasing = TRUE), Q=upset, type=type))
      ESdown <- apply(mat, 2, function(x) 
        .enrichScore(sigvec=sort(x, decreasing = TRUE), Q=downset, type=type)) 
      ESout1 <- ifelse(sign(ESup) != sign(ESdown), (ESup - ESdown)/2, 0)
      ## When only upset is provided
    } else if(length(upset)>0 & length(downset)==0) {
      ESup <- apply(mat, 2, function(x) 
        .enrichScore(sigvec=sort(x, decreasing = TRUE), Q=upset, type=type))
      ESout1 <- ESup
      ## When only downset is provided
    } else if(length(upset)==0 & length(downset)>0) {
      ESdown <- apply(mat, 2, function(x) 
        .enrichScore(sigvec=sort(x, decreasing = TRUE), Q=downset, type=type))
      ESout1 <- -ESdown
      ## When none are provided (excluded by input validity check already!)
    }
    ESout <- c(ESout, ESout1)
  }
  
  ## Assmble output 
  if(output=="esonly") {
    return(ESout)
  }
  if(output=="all") {
    resultDF <- .lincsScores(esout=ESout, upset=upset, downset=downset, 
        minTauRefSize=minTauRefSize, ES_NULL=ES_NULL, tau=tau)
  }
  if(!is.na(sortby)) {
    resultDF <- resultDF[order(abs(resultDF[,sortby]), decreasing=TRUE), ]
  } else {
    resultDF <- resultDF
  }
  row.names(resultDF) <- NULL
  return(resultDF)
}

#' Generate ES null distribution with specified number of random queryies 
#' against the reference database (se) for computing nominal P-values for ES
#' in the `gess_lincs` result.
#' 
#' @title Generate ES null distribution with random queries
#' @param se SummarizedExperiment object represents reference database
#' @param N_queries number of random queries
#' @param dest_ES_NULL_path file path to the generated "ES_NULL.txt" file
#' @return ES_NULL.txt file
#' @importFrom utils write.table
#' @examples 
#' dbpath = system.file("extdata", "sample_db", package="signatureSearch")
#' se = HDF5Array::loadHDF5SummarizedExperiment(dbpath)
#' randQueryES(se=se, N_queries=5, dest_ES_NULL_path="ES_NULL.txt")
#' unlink("ES_NULL.txt")
#' @seealso \code{\link{gess_lincs}}
#' @references 
#' Subramanian, A., Narayan, R., Corsello, S. M., Peck, D. D., Natoli, T. E., 
#' Lu, X., … Golub, T. R. (2017). A Next Generation Connectivity Map: L1000 
#' Platform and the First 1,000,000 Profiles. Cell, 171(6), 1437–1452.e17. 
#' \url{https://doi.org/10.1016/j.cell.2017.10.049}
#' @export

randQueryES <- function(se, N_queries=1000, dest_ES_NULL_path) {
  ## Create list of random queries
  idnames <- rownames(se)
  query_list <- randQuerySets(id_names=idnames, N_queries=N_queries, 
                              set_length=150)
  ## Define vapply function
  f <- function(x, query_list, se) {
      esout <- lincsEnrich(se, upset=query_list[[x]]$up, 
                           downset=query_list[[x]]$down, sortby=NA, 
                           output="esonly", type=1)
      names(esout) <- colData(se)$pert_cell_factor
      # message("Random query ", sprintf("%04d", x), 
      #         " has been searched against reference database")
      wtcs = esout
  }
  myMA <- vapply(seq(along=query_list), f, query_list, se, 
                 FUN.VALUE=double(ncol(se)))
  colnames(myMA) <- names(query_list)
  ## Collect results in frequency table with 3 diget accuracy
  esMA <- data.frame(WTCS=as.character(round(rev(seq(-1, 1, by=0.001)), 3)), 
                     Freq=0, stringsAsFactors=FALSE) 
  freq <- table(round(as.numeric(as.matrix(myMA),3),3)) 
  ## processes entire myMA data.frame
  freq <- freq[as.character(esMA[,1])]
  freq[is.na(freq)] <- 0
  esMA[,"Freq"] <- as.numeric(esMA[,"Freq"]) + as.numeric(freq)
  write.table(esMA, file=dest_ES_NULL_path, quote=FALSE, 
              row.names=FALSE, sep="\t")
}
